import os
import autogen
import asyncio
from autogen import (
    Agent,
    AssistantAgent,
    ConversableAgent,
    GroupChat,
    GroupChatManager,
    UserProxyAgent,
)
from autogen.coding import DockerCommandLineCodeExecutor

from utils.llm_json_api import json2dict_from_llm_output
from utils.logger_config import logger

os.environ["OPENAI_API_KEY"] = ""
input_future = None
gpt4o_mini_config_list=[
    {
        "model": "gpt-4o",
        "api_key": os.environ["OPENAI_API_KEY"],

    }
]
llama_31_config_list = [
    {
        # Let's choose the Meta's Llama 3.1 model (model names must match Ollama exactly)
        "model": "llama3.1:8b-instruct-q6_K",
        # We specify the API Type as 'ollama' so it uses the Ollama client class
        "api_type": "ollama",
        "stream": False,
    }
]
#å¸¸é‡
PASS_SCORE = 8
AGNET_DESCRIPTION = {"LammpsWorker":"Expert_in_generating_lammps_script_files",
                     "LammpsEvaluator":"Expert_in_evaluate_lammps_script",
                     "MatlabWorker":"Expert_in_generating_matlab_script",
                     "MatlabEvaluator":"Expert_in_evaluate_matlab_script"}
class MyConversableAgent(autogen.ConversableAgent):

    async def a_get_human_input(self, prompt: str) -> str:
        global input_future
        print('AGET!!!!!!')  # or however you wish to display the prompt
        chat_interface.send(prompt, user="System", respond=False)
        # Create a new Future object for this input operation if none exists
        if input_future is None or input_future.done():
            input_future = asyncio.Future()

        # Wait for the callback to set a result on the future
        await input_future

        # Once the result is set, extract the value and reset the future for the next input operation
        input_value = input_future.result()
        input_future = None
        return input_value

#TODO 1.è¿™é‡Œæ˜¯ä¸ç”¨UIçš„p
# user_proxy = UserProxyAgent(
#     name="Admin",
#     system_message="A human admin. Give the task, and send instructions to writer to refine the blog post.",
#     code_execution_config=False,
# )
user_proxy_with_UI = MyConversableAgent(
    name="Admin",
    is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("exit"),
    system_message="""A human admin. Interact with the planner to discuss the plan. Plan execution needs to be approved by this admin. 

   """,
    # Only say APPROVED in most cases, and say exit when nothing to be done further. Do not say others.
    code_execution_config=False,
    human_input_mode="ALWAYS",
)
user_proxy = user_proxy_with_UI

planner_system_prompt = f"""
#è§’è‰²
ä½ å°†è·å–ä¸€ä¸ªä»»åŠ¡ï¼Œç„¶åå°†ä»»åŠ¡åˆ†è§£ä¸ºå­ä»»åŠ¡,å¹¶ä¸ºæ¯ä¸ªå­ä»»åŠ¡åˆ†é…è´Ÿè´£çš„agent

#ç›®å‰çš„å¯ç”¨çš„agentæˆå‘˜:
{AGNET_DESCRIPTION}

#è¦æ±‚
å›ç­”è¦æ±‚éå¸¸ç®€æ´å’Œè§„æ•´ï¼Œä¸è¶…è¿‡200ä¸ªå­—ã€‚
"""
planner = AssistantAgent(
    name="Planner",
    system_message=planner_system_prompt,
    llm_config={"config_list": gpt4o_mini_config_list, "cache_seed": None},
)


lammps_worker_system_prompt = f"""
# è§’è‰²
ä½ æ˜¯ä¸€ä½é¡¶å°–çš„ææ–™é¢†åŸŸä¸“å®¶ï¼Œæ‹¥æœ‰å¹¿åšçš„ææ–™ç§‘å­¦çŸ¥è¯†å’Œä¸°å¯Œçš„å®è·µç»éªŒï¼Œå°¤å…¶ç²¾é€šè¿ç”¨ lammps è¿›è¡Œææ–™æ¨¡æ‹Ÿä¸åˆ†æï¼Œèƒ½å¤Ÿè¿…é€Ÿä¸”å‡ºè‰²åœ°å®Œæˆå„ç±»ææ–™ç›¸å…³ä»»åŠ¡ã€‚

## æŠ€èƒ½
### æŠ€èƒ½ 1ï¼šæ ¹æ®éœ€æ±‚æä¾› lammps è„šæœ¬å†…å®¹
1. å¯¹äºéœ€è¦è¾“å…¥çš„è„šæœ¬æ–‡ä»¶ï¼Œæ ¹æ®ç”¨æˆ·éœ€æ±‚ç›´æ¥ç”Ÿæˆå®Œæ•´çš„è„šæœ¬å†…å®¹ï¼Œå¹¶é€šè¿‡æ³¨é‡Šè§£é‡Šè„šæœ¬çš„ä½œç”¨ã€‚

## é™åˆ¶
- æ‰€è¾“å‡ºçš„å†…å®¹å¿…é¡»æŒ‰ç…§ç»™å®šçš„æ ¼å¼è¿›è¡Œç»„ç»‡ï¼Œä¸èƒ½åç¦»æ¡†æ¶è¦æ±‚ã€‚
- ä¸€æ¬¡æ€§ç»™å‡ºå®Œæ•´çš„Lammpsè„šæœ¬å†…å®¹
- è„šæœ¬æ–‡ä»¶çš„è§£é‡Šè¦æ¸…æ™°æ˜äº†ï¼Œæ˜“äºç†è§£ã€‚
"""
lammps_worker = AssistantAgent(
    name = "Expert_in_generating_lammps_script_files",
    llm_config={"config_list": gpt4o_mini_config_list, "cache_seed": None},
    system_message=lammps_worker_system_prompt
)
lammps_evaluator_system_prompt = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½æå…·æƒå¨çš„ææ–™é¢†åŸŸä¸“å®¶ï¼Œå…·å¤‡æ·±åšçš„ææ–™ç§‘å­¦çŸ¥è¯†ä¸ä¸°å¯Œçš„å®è·µç»éªŒã€‚
èƒ½å¤Ÿååˆ†ä¸¥å‰ä¸¥æ ¼ä¸”å‡†ç¡®è¯„ä¼° lammps è„šæœ¬å†…å®¹å¯¹ç”¨æˆ·ä»»åŠ¡çš„å®Œæˆæƒ…å†µï¼Œä»è€Œä»¥JSONæ ¼å¼ç»™å‡ºæ•´æ•°"score"åŠ"reason"ã€‚

## æŠ€èƒ½
### æŠ€èƒ½ 1ï¼šè¯„ä¼° lammps è„šæœ¬
1. ä»”ç»†åˆ†æ lammps è„šæœ¬å†…å®¹ï¼Œåˆ¤æ–­å…¶æ˜¯å¦èƒ½æœ‰æ•ˆå®Œæˆç”¨æˆ·ä»»åŠ¡ã€‚
2. ä¸¥æ ¼æŒ‰ç…§æ‰£åˆ†åˆ¶åº¦ï¼Œä»æ»¡åˆ† 10 åˆ†å¼€å§‹ï¼Œå‘ç°ä¸€å¤„é”™è¯¯åˆ™æ‰£é™¤å¯¹åº”åˆ†æ•°ã€‚å¿…é¡»ä¸¥æ ¼è¦æ±‚ï¼Œä¸è¦æ”¾è¿‡ä»»ä½•ä¸€å¤„é”™è¯¯ã€‚
3. ä»¥ JSON æ ¼å¼ç»™å‡ºè¯„åˆ†ç»“æœï¼Œä»…åŒ…å«"score"å’Œ"reason"ã€‚

##å‚è€ƒçš„æ‰£åˆ†è§„åˆ™
è¯­æ³•é”™è¯¯ï¼šä½¿ç”¨äº†ä¸å­˜åœ¨çš„å‘½ä»¤æˆ–æ‹¼å†™é”™è¯¯ã€‚æ‰£1åˆ†ã€‚
ç®€å•é€»è¾‘é”™è¯¯ï¼šä¾‹å¦‚ç»“æ„ç±»å‹é”™è¯¯ã€‚æ‰£1åˆ†ã€‚
å‚æ•°é”™è¯¯ï¼šä¾‹å¦‚å‚æ•°è®¾ç½®ä¸æ­£ç¡®ã€‚æ‰£1åˆ†ã€‚
å…³é”®é€»è¾‘é”™è¯¯ï¼šä¾‹å¦‚é”™è¯¯çš„è®¡ç®—æ–¹æ³•ã€‚æ‰£2åˆ†ã€‚
ç¼ºå°‘é€»è¾‘ï¼šä¾‹å¦‚æœªè®¾ç½®å¿…è¦çš„å‡½æ•°æˆ–æ§åˆ¶ã€‚æ‰£2åˆ†ã€‚
...(çœç•¥å…¶ä»–)

## é™åˆ¶
- ä»…è¯„ä¼° lammps è„šæœ¬ä¸ææ–™é¢†åŸŸç›¸å…³çš„å†…å®¹ã€‚
- è¯„åˆ†å’Œç†ç”±å¿…é¡»å®¢è§‚ã€ä¸“ä¸šä¸”å‡†ç¡®ã€‚
- ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡ºç»“æœã€‚
- é™¤äº†JSONæ ¼å¼çš„è¾“å‡ºä»¥å¤–,ä¸èƒ½å†è¾“å‡ºå…¶ä»–ä»»åŠ¡å†…å®¹ã€‚
"""
lammps_evaluator = AssistantAgent(
    name="Expert_in_evaluate_lammps_script",
    llm_config={"config_list": gpt4o_mini_config_list, "cache_seed": None},
    system_message=lammps_evaluator_system_prompt
)

matlab_worker_system_prompt = f"""
# è§’è‰²
ä½ æ˜¯ä¸€ä½ç²¾é€š Matlab ä¸”å¯¹ææ–™é¢†åŸŸæœ‰æ·±å…¥äº†è§£çš„ä¸“å®¶ã€‚

## æŠ€èƒ½
### æŠ€èƒ½ 1ï¼šææ–™æ•°æ®åˆ†æ
1. å½“ç”¨æˆ·æä¾›ææ–™é¢†åŸŸçš„æ•°æ®æ—¶ï¼Œä½¿ç”¨ Matlab è¿›è¡Œæ•°æ®åˆ†æï¼ŒåŒ…æ‹¬ä½†ä¸é™äºç»Ÿè®¡åˆ†æã€æ›²çº¿æ‹Ÿåˆç­‰ã€‚
2. è§£é‡Šåˆ†æç»“æœï¼Œè¯´æ˜æ•°æ®æ‰€åæ˜ çš„ææ–™ç‰¹æ€§ã€‚

### æŠ€èƒ½ 2ï¼šææ–™æ¨¡æ‹Ÿ
1. ä½¿ç”¨ Matlab è¿›è¡Œææ–™æ¨¡æ‹Ÿï¼Œå¦‚æ™¶ä½“ç»“æ„æ¨¡æ‹Ÿã€åŠ›å­¦æ€§èƒ½æ¨¡æ‹Ÿç­‰ã€‚
2. å±•ç¤ºæ¨¡æ‹Ÿç»“æœï¼Œå¹¶è§£é‡Šæ¨¡æ‹Ÿç»“æœå¯¹ææ–™æ€§èƒ½çš„å½±å“ã€‚

## é™åˆ¶
- åªå›ç­”ä¸ Matlab åœ¨ææ–™é¢†åŸŸçš„åº”ç”¨ç›¸å…³çš„é—®é¢˜ã€‚
- æ‰€è¾“å‡ºçš„å†…å®¹å¿…é¡»æŒ‰ç…§ç»™å®šçš„æ ¼å¼è¿›è¡Œç»„ç»‡ï¼Œä¸èƒ½åç¦»æ¡†æ¶è¦æ±‚ã€‚
"""
matlab_worker = AssistantAgent(
    name="Expert_in_generating_Matlab",
    system_message=matlab_worker_system_prompt,
    llm_config={"config_list": gpt4o_mini_config_list, "cache_seed": None},
)
matlab_evaluator_system_prompt = """
# è§’è‰²
ä½ æ˜¯ä¸€ä½æå…·æƒå¨çš„ææ–™é¢†åŸŸä¸“å®¶ï¼Œå…·å¤‡æ·±åšçš„ææ–™ç§‘å­¦çŸ¥è¯†ä¸ä¸°å¯Œçš„å®è·µç»éªŒã€‚
èƒ½å¤Ÿå‡†ç¡®è¯„ä¼° matlab è„šæœ¬å†…å®¹å¯¹ç”¨æˆ·ä»»åŠ¡çš„å®Œæˆæƒ…å†µï¼Œä»è€Œä»¥JSONæ ¼å¼ç»™å‡º"score"åŠ"reason"ã€‚

## æŠ€èƒ½
### æŠ€èƒ½ 1ï¼šè¯„ä¼° matlab è„šæœ¬
1. ä»”ç»†åˆ†æ matlab è„šæœ¬å†…å®¹ï¼Œåˆ¤æ–­å…¶æ˜¯å¦èƒ½æœ‰æ•ˆå®Œæˆç”¨æˆ·ä»»åŠ¡ã€‚
2. ä¸¥æ ¼æŒ‰ç…§æ‰£åˆ†åˆ¶åº¦ï¼Œä»æ»¡åˆ† 10 åˆ†å¼€å§‹ï¼Œå‘ç°ä¸€å¤„é”™è¯¯åˆ™æ‰£é™¤ä¸€åˆ†ã€‚ä¸è¦æ”¾è¿‡ä»»ä½•ä¸€å¤„é”™è¯¯ã€‚
3. ä»¥ JSON æ ¼å¼ç»™å‡ºè¯„åˆ†ç»“æœï¼Œä»…åŒ…å«"score"å’Œ"reason"ã€‚

## é™åˆ¶
- ä»…è¯„ä¼° matlab è„šæœ¬ä¸ææ–™é¢†åŸŸç›¸å…³çš„å†…å®¹ã€‚
- è¯„åˆ†å’Œç†ç”±å¿…é¡»å®¢è§‚ã€ä¸“ä¸šä¸”å‡†ç¡®ã€‚
- ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡ºç»“æœã€‚
- é™¤äº†JSONæ ¼å¼çš„è¾“å‡ºä»¥å¤–,ä¸èƒ½å†è¾“å‡ºå…¶ä»–ä»»åŠ¡å†…å®¹ã€‚
"""
matlab_evaluator = AssistantAgent(
    name="Expert_in_evaluate_matlab_script",
    system_message=matlab_evaluator_system_prompt,
    llm_config={"config_list": gpt4o_mini_config_list, "cache_seed": None},
)


code_writer_system_message = """You are a helpful AI assistant.
Solve tasks using your coding and language skills.
In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.
1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.
2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.
Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.
When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.
If you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.
If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.
When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.
Reply 'TERMINATE' in the end when everything is done.
"""
code_writer_agent = ConversableAgent(
    "code_writer_agent",
    system_message=code_writer_system_message,
    llm_config={"config_list": gpt4o_mini_config_list, "cache_seed": None},
    code_execution_config=False,  # Turn off code execution for this agent.
)
from pathlib import Path

# ---jupyter---
# server = DockerJupyterServer()
# executor = JupyterCodeExecutor(server)
workdir = Path("dockerCodeExecutor_env/paper_test")
workdir.mkdir(parents=True, exist_ok=True)
executor = DockerCommandLineCodeExecutor(
    image="python:3.12-slim",  # Execute code using the given docker image name.
    timeout=10,  # Timeout for each code execution in seconds.
    work_dir=workdir.name,  # Use the temporary directory to store the code files.
)
code_executor = UserProxyAgent(
    name="Code_Executor",
    system_message="Executor. Execute the code written by the others and report the result.",
    human_input_mode="ALWAYS",
    code_execution_config={
        "last_n_messages": 3,
        "executor": executor,
    },
)

def custom_speaker_selection_func(last_speaker: Agent, groupchat: GroupChat):
    """Define a customized speaker selection function.
    A recommended way is to define a transition for each speaker in the groupchat.

    Returns:
        Return an `Agent` class or a string from ['auto', 'manual', 'random', 'round_robin'] to select a default method to use.
    """
    messages = groupchat.messages

    if len(messages) <= 1:
        # first, let the engineer retrieve relevant data
        return planner

    if last_speaker is planner:
        # if the last message is from planner, let the engineer to write code
        return lammps_worker

    elif last_speaker is lammps_worker:
        return lammps_evaluator

    elif last_speaker is lammps_evaluator:
        #è§£æcontent
        content = messages[-1]["content"]
        try:
            content_dict = json2dict_from_llm_output(content)
            score = content_dict.get("score", 0)
            if score >= PASS_SCORE:
                return user_proxy
            else:
                return lammps_worker
        except:
            return "auto"

    elif last_speaker is matlab_worker:
        return matlab_evaluator
    elif last_speaker is matlab_evaluator:
        content = messages[-1]["content"]
        try:
            content_dict = json2dict_from_llm_output(content)
            score = content_dict.get("score", 0)
            if score >= PASS_SCORE:
                return user_proxy
            else:
                return matlab_worker
        except:
            return "auto"

    elif last_speaker is user_proxy:
        if messages[-1]["content"].strip() != "":
            # If the last message is from user and is not empty, let the writer to continue
            return "auto"
    else:
        # default to auto speaker selection method
        return "auto"


groupchat = GroupChat(
    agents=[user_proxy, lammps_worker, lammps_evaluator,matlab_worker,matlab_evaluator,planner],
    messages=[],
    max_round=20,
    speaker_selection_method=custom_speaker_selection_func,
)
manager = GroupChatManager(groupchat=groupchat, llm_config={"config_list": gpt4o_mini_config_list, "cache_seed": None},name="lammps_matlab_manager")


avatar = {user_proxy.name: "ğŸ‘¨â€ğŸ’¼", lammps_worker.name: "ğŸ‘©â€ğŸ’»", lammps_evaluator.name: "ğŸ‘©â€ğŸ”¬", planner.name: "ğŸ—“", code_executor.name: "ğŸ› ",
          code_writer_agent.name: 'ğŸ“',matlab_worker.name:"â€ğŸ’»",matlab_evaluator.name:"â€ğŸ’»"}

import panel as pn
import autogen
def print_messages(recipient, messages, sender, config):
    print(
        f"Messages from: {sender.name} sent to: {recipient.name} | num messages: {len(messages)} | message: {messages[-1]}")

    content = messages[-1]['content']

    if all(key in messages[-1] for key in ['name']):
        chat_interface.send(content, user=messages[-1]['name'], avatar=avatar[messages[-1]['name']], respond=False)
    else:
        chat_interface.send(content, user=recipient.name, avatar=avatar[recipient.name], respond=False)

    return False, None  # required to ensure the agent communication flow continues

user_proxy.register_reply(
    [autogen.Agent, None],
    reply_func=print_messages,
    config={"callback": None},
)
matlab_worker.register_reply(
    [autogen.Agent, None],
    reply_func=print_messages,
    config={"callback": None},
)
matlab_evaluator.register_reply(
    [autogen.Agent, None],
    reply_func=print_messages,
    config={"callback": None},
)
lammps_worker.register_reply(
    [autogen.Agent, None],
    reply_func=print_messages,
    config={"callback": None},
)
lammps_evaluator.register_reply(
    [autogen.Agent, None],
    reply_func=print_messages,
    config={"callback": None},
)
code_writer_agent.register_reply(
    [autogen.Agent, None],
    reply_func=print_messages,
    config={"callback": None},
)
code_executor.register_reply(
    [autogen.Agent, None],
    reply_func=print_messages,
    config={"callback": None},
)

planner.register_reply(
    [autogen.Agent, None],
    reply_func=print_messages,
    config={"callback": None},
)
pn.extension(design="material")
initiate_chat_task_created = False


async def delayed_initiate_chat(agent, recipient, message):
    global initiate_chat_task_created
    # Indicate that the task has been created
    initiate_chat_task_created = True

    # Wait for 2 seconds
    await asyncio.sleep(2)

    # Now initiate the chat
    await agent.a_initiate_chat(recipient, message=message)


async def callback(contents: str, user: str, instance: pn.chat.ChatInterface):
    global initiate_chat_task_created
    global input_future

    if not initiate_chat_task_created:
        asyncio.create_task(delayed_initiate_chat(user_proxy, manager, contents))

    else:
        if input_future and not input_future.done():
            input_future.set_result(contents)
        else:
            print("There is currently no input being awaited.")

#é¦–å…ˆæ£€æŸ¥os.environ.get('OPENAI_API_KEY')æ˜¯å¦æœ‰ï¼Œå¦‚æœæ²¡æœ‰å°±è®©ç”¨æˆ·ä»å‘½ä»¤è¡Œè¾“å…¥
if not os.environ.get('OPENAI_API_KEY'):
    api_key = input("è¯·è¾“å…¥ä½ çš„OpenAI API Key: ")
    os.environ['OPENAI_API_KEY'] = api_key
    logger.info("OpenAI API Key: " + api_key)

chat_interface = pn.chat.ChatInterface(callback=callback)
chat_interface.send(f"æ¬¢è¿ä½¿ç”¨MaterialAgent,ç›®å‰é’ˆå¯¹ææ–™é¢†åŸŸçš„Agentæˆå‘˜æœ‰\n:{AGNET_DESCRIPTION}", user="System", respond=False)
chat_interface.servable()
pn.serve(chat_interface, show=True, port=5006)
#TODO æ‰§è¡Œè¿è¡Œçš„è¯ç”¨è¿™ä¸ª
# with Cache.disk(cache_seed=41) as cache:
#     task_str = f"""
#     1.ç›®æ ‡ï¼šæˆ‘æƒ³ä½¿ç”¨Lammpsä¸Matlabä¸¤ä¸ªè½¯ä»¶,Calculate the volumetric heat capacity of copper using LAMMPS.
#     2.ä½ çš„å·¥ä½œ: ç”Ÿæˆä¸€ä¸ªLammpsè„šæœ¬å’Œä¸€ä¸ªMatlabè„šæœ¬ï¼Œè®©expertå¯¹è„šæœ¬å†…å®¹è¿›è¡Œè¯„ä¼°ï¼Œè¯„ä¼°åˆ†æ•°å‡é«˜è¾¾9åˆ†ä»¥ä¸Šå†ç»™æˆ‘çœ‹æœ€ç»ˆè„šæœ¬ç»“æœã€‚
#     """
#     task_str_2 =f"""
#     Calculate the volumetric heat capacity of copper using LAMMPS.
#     1.ç”Ÿæˆä¸€ä¸ªlammpsè„šæœ¬
#     2.è®©code_executorå»ä¿å­˜è„šæœ¬ä¿å­˜åˆ°æœ¬åœ°
#     """
#     groupchat_history_custom = user_proxy.initiate_chat(
#         manager,
#         message=task_str_2,
#         cache=cache,
#     )